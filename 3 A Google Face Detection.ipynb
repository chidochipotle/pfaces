{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Vision Example\n",
    "\n",
    "First make sure you set up your google account with a [Cloud Vision Project](https://cloud.google.com/vision/docs/getting-started##set_up_your_project).  Then you need to enter billing details (to verify you are human) and set up credentials.  This will download a `.json` file.\n",
    "\n",
    "Before you work in this notebook, you have to do this to let python know about your [application default credentials](https://cloud.google.com/vision/docs/getting-started#adc).  So put the `.json` file in this folder (but do not commit it into git!).  Then `export GOOGLE_APPLICATION_CREDENTIALS=<path to your json file>`.\n",
    "\n",
    "This code is based on [Google's face detection tutorial](https://cloud.google.com/vision/docs/face-tutorial).\n",
    "\n",
    "## Requirements\n",
    "```\n",
    "google-api-python-client==1.5.0\n",
    "Pillow==3.1.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from googleapiclient import discovery\n",
    "import httplib2\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import base64\n",
    "import os\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from IPython import display\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize Database\n",
    "client = MongoClient()\n",
    "db = client.download\n",
    "coll = db.photos\n",
    "photo = { \"photo_name\": \"http:////\",\n",
    "         \"faces\": \"test\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up a connection to the Google service\n",
    "DISCOVERY_URL='https://{api}.googleapis.com/$discovery/rest?version={apiVersion}'\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "service = discovery.build('vision', 'v1', credentials=credentials, discoveryServiceUrl=DISCOVERY_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Found 10 faces\n"
     ]
    }
   ],
   "source": [
    "# load the input images\n",
    "img_path=\"/home/resonon/tutorial/faces/random10\"\n",
    "batch_request = []\n",
    "image_pathnames = []\n",
    "# create a dictionary with all the image_pathname\n",
    "images = [os.path.join(img_path, name) for name in os.listdir(img_path) ]\n",
    "\n",
    "# get google vision results for batch \n",
    "for image_pathname in images:\n",
    "    image = open(image_pathname,'rb')\n",
    "    image_content = image.read()\n",
    "    # fire off request for face detection from Google\n",
    "    json_request = {\n",
    "        'image': {'content': base64.b64encode(image_content)},\n",
    "        'features': [{ 'type': 'FACE_DETECTION','maxResults': 4, }]}\n",
    "    batch_request.append(json_request)\n",
    "    image_pathnames.append(image_pathname)\n",
    "    request = service.images().annotate(body={'requests': batch_request,})\n",
    "    response = request.execute()\n",
    "print(len(image_pathnames))\n",
    "\n",
    "print('Found %s face%s' % (len(response['responses']), \n",
    "                           '' if len(response['responses']) == 1 else 's'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def highlight_crop_faces(image_name, face):\n",
    "    \"\"\"Draws a polygon around the faces, then saves to output_filename.\n",
    "\n",
    "\n",
    "Args:\n",
    "      image: a file containing the image with the faces.\n",
    "      faces: a list of faces found in the file. This should be in the format\n",
    "          returned by the Vision API.\n",
    "      output_filename: the name of the image file to be created, where the faces\n",
    "          have polygons drawn around them.\n",
    "    \"\"\"\n",
    "    print(1)\n",
    "    im = Image.open(image_name)\n",
    "    im_copy = im.copy()\n",
    "    draw = ImageDraw.Draw(im_copy)\n",
    "    \n",
    "    home_path, filename = '/'.join(str(image).split('/')[:-2]),  str(image).split('/')[-1]\n",
    "    cropped_dir = '/'.join(str(image).split('/')[-2:-1])+\"_cropped\"\n",
    "    highligthed_dir = '/'.join(str(image).split('/')[-2:-1])+\"_highlighted\"\n",
    "\n",
    "    cropped_pathname = []\n",
    "    if face.has_key('faceAnnotations'):\n",
    "        for i in range(len(face['faceAnnotations'])):\n",
    "            vertices = face['faceAnnotations'][i]['fdBoundingPoly']['vertices']\n",
    "            #print(vertices)\n",
    "            box = [(v['x'], v['y']) for v in vertices if v.has_key('x') and v.has_key('y')]\n",
    "\n",
    "            prefix = ''\n",
    "            if len(face['faceAnnotations']) > 1:\n",
    "                prefix=str(i)\n",
    "                print(i)\n",
    "            if len(box) == 4:\n",
    "                cropped_pathname = os.path.join(home_path, cropped_dir, prefix + filename)\n",
    "                cropped_pathnames.append(os.path.join(home_path, cropped_dir, prefix + filename))\n",
    "                cropped = im.crop((box[0][0], box[1][1],box[2][0], box[2][1])) \n",
    "                cropped.save(cropped_pathname, 'jpeg')\n",
    "\n",
    "                highligthed_pathname = os.path.join(home_path, highligthed_dir, prefix + filename)\n",
    "                draw.line(box + [box[0]], width=5, fill='#00ff00')                \n",
    "                im_copy.save(highligthed_pathname, 'jpeg')\n",
    "            else:\n",
    "                print(face['faceAnnotations'][i])\n",
    "        #    im.save(highligthed_pathname, 'jpeg')\n",
    "\n",
    "    del draw\n",
    "    del im_copy\n",
    "    return(cropped_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 1\n",
      ". 1\n",
      ". 1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      ". 1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      ". 1\n",
      ". 1\n",
      ". 1\n",
      ". 1\n",
      ". 1\n",
      ". 1\n"
     ]
    }
   ],
   "source": [
    "cropped_pathnames = []\n",
    "for image, face in zip(image_pathnames, response['responses']):\n",
    "    print(\".\"),\n",
    "    cropped_pathnames.append(highlight_crop_faces(image, face))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(response['responses'])):\n",
    "    if response['responses'][i].has_key('faceAnnotations'):\n",
    "        print len(response['responses'][i]['faceAnnotations']),#[0]['fdBoundingPoly']['vertices']\n",
    "#response['responses']['faceAnnotations'][0]['fdBoundingPoly']['vertices']\n",
    "#response['responses']['faceAnnotations'][0]['fdBoundingPoly']['vertices']\n",
    "#response['responses']['faceAnnotations'][0]['fdBoundingPoly']['vertices']\n",
    "#response['responses']['faceAnnotations'][0]['fdBoundingPoly']['vertices']\n",
    "#response['responses']['faceAnnotations'][0]['fdBoundingPoly']['vertices']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for image_pathname in images:\n",
    "#    highlight_crop_faces(image_pathname, face_filename, result['faceAnnotations'], );\n",
    "#    for annotation in result['faceAnnotations']:\n",
    "#       for emotion in ['joy','sorrow','surprise','anger']:\n",
    "#            print \"%s: %s\" % (emotion, annotation[emotion+'Likelihood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "for result in response['responses']:\n",
    "    if not result.has_key(\"faceAnnotations\"):\n",
    "        print(c, \"no face\")\n",
    "    else:\n",
    "        print(c),\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cropped_pathnames = []\n",
    "#for image, face, candidate in zip(image_pathnames, response['responses']):\n",
    "#    print(image, face, candidate)\n",
    "#    cropped_pathnames.append(highlight_crop_faces(image, face))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#highlight_crop_faces(image_pathnames[0], response['responses'][0], candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print out emotions for each one\n",
    "likelyhood_to_number = {\"UNKNOWN\":-1,\n",
    "\"VERY_UNLIKELY\":1,\n",
    "\"UNLIKELY\":2,\n",
    "\"POSSIBLE\":3,\n",
    "\"LIKELY\":4,\n",
    "\"VERY_LIKELY\":5}\n",
    "for result, candidate, image, cropped_image in zip(response['responses'], candidates, image_pathnames, cropped_pathnames):\n",
    "    print(candidate)\n",
    "    display.display(display.Image(filename=image,  width=100), display.Image(filename=cropped_image,  width=100))\n",
    "    if result.has_key('faceAnnotations'):\n",
    "        for faceAnnotations in result['faceAnnotations']:\n",
    "            emotions = []\n",
    "            for emotion, smiley in zip(['joy', 'sorrow', 'surprise', 'anger'], [\"ðŸ˜ƒ\", \"ðŸ˜¢\", \"ðŸ˜®\", \"ðŸ˜¡\"]):\n",
    "                 emotions.append(emotion + \":\" + str(likelyhood_to_number[faceAnnotations[emotion+\"Likelihood\"]]))\n",
    "            print(\" \".join(emotions))\n",
    "    else:\n",
    "        print(\"Image processing error!\")\n",
    "    print(\"\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ann = response['responses'][6]['faceAnnotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = response['responses'][7]\n",
    "r.has_key('faceAnnotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(len(response['responses']), len(candidates), len(image_pathnames), len(cropped_pathnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response['responses'][6]['faceAnnotations'][0]['fdBoundingPoly']['vertices']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html\n",
    "\n",
    "http://scikit-learn.org/stable/datasets/labeled_faces.html\n",
    "\n",
    "http://gaelvaroquaux.github.io/scikit-learn-tutorial/putting_together.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
